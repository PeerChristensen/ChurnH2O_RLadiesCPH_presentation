---
title: "Churn Prediction with H2O AutoML"
author: "Peer Christensen"
date: "R-Ladies Copenhagen, 2019-12-05"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=F,message=F,cache=F)
```

class: inverse, center, middle

# EDA

---

```{r echo = F}
library(tidyverse)

df <- read_csv("new_churn_training.csv") %>%
  mutate(Perm_anyperm = factor(Perm_anyperm),
         Churned30 = factor(Churned30),
       Perm_recommendations = factor(Perm_recommendations),
       Perm_newsletter = factor(Perm_newsletter),
       MatasUser = factor(MatasUser),
       CoopUser = factor(CoopUser)) %>%
  mutate_if(is.character,factor) %>%
  select(-IsFree,-PremiumEbookRatio) %>%
  drop_na() %>%
  filter(DaysSinceLatestSignup > 30) 

# Store the identifier for later
Customer_Key <- df$Customer_Key

df <- df %>% select(-Customer_Key)
```

### Rows and columns

```{r}
dim(df)
```

--

### Checking for target class (im)balance

```{r}
table(df$Churned30)
           
round(prop.table(table(df$Churned30)),3)*100
```

---

```{r}
names(df[1:50])
```

---

## Correlation funnel

```{r}
library(tidyverse)
library(correlationfunnel)

df_binned <- df %>%
  select(-DateStatus) %>%
  binarize(n_bins = 5, thresh_infreq = 0.01, name_infreq = "OTHER", one_hot = TRUE)

df_churn_corr <- df_binned %>%
  correlate(Churned30__1)

```

---

```{r}
df_churn_corr %>%
  plot_correlation_funnel()
```

---

```{r}
df_churn_corr %>%
  arrange(desc(correlation)) %>%
  top_n(15) %>%
  plot_correlation_funnel()
```

---

## Plotting Churn against the best predictors

```{r}
red   <- "#c51924"
blue  <- "#028ccc"

top_features <- df_churn_corr %>%
  top_n(15) %>%
  filter(feature != "Churned30") %>%
  pull(feature) %>% 
  as.vector()

numeric_features <- df %>%
  select(top_features) %>%
  select_if(is.numeric) %>% 
  names()

categorical_features <- df %>%
  select(top_features) %>%
  select_if(is.factor) %>% 
  names()
```

---

## Churn ~ Numeric variables

```{r}
p1 <- df %>%
  select(Churned30,numeric_features) %>%
  gather(variable, value,-Churned30) %>%
  ggplot(aes(x=value, fill = Churned30,colour=Churned30)) +
  facet_wrap(~variable,scales="free",ncol=3) +
  geom_density(alpha = 0.5) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 1),
        legend.position = "top",
        legend.text = element_text(size=18),
        legend.title = element_text(size=18)) +
  scale_fill_manual(values=c(red,blue)) +
  scale_colour_manual(values=c(red,blue))
```

---

```{r}
p1
```

---

## Churn ~ Categorical variables

```{r}
p2 <- df %>%
  select(Churned30,categorical_features) %>%
  gather(variable, value,-Churned30) %>%
  mutate(value = ifelse(is.na(value),"na",value)) %>%
  count(Churned30, variable, value) %>%
  ggplot(aes(x = reorder(value,-n), y = n, fill = Churned30, color = Churned30)) +
  facet_wrap(~ variable, ncol = 3, scales = "free") +
  geom_bar(stat = "identity", alpha = 0.5) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "top",
        legend.text = element_text(size=18),
        legend.title = element_text(size=18)) +
  scale_fill_manual(values=c(red,blue)) +
  scale_colour_manual(values=c(red,blue))
```

---

```{r}
p2
```

---

class: inverse, center, middle

# Feature engineering

---

### Calculate order size

```{r}
df$AverageOrderSize <- df$TotalNetRevenue / df$TotalOrderCount
```

### Signup month

```{r}
library(lubridate)

# Get the current date
today <- max(as.Date(df$DateStatus)) 

# Get the signup date
DateLatestSignup <- today - days(df$DaysSinceLatestSignup)

# Find the signup month
df$DateLatestSignup_month <- lubridate::month(DateLatestSignup)

df <- df %>% select(-DateStatus)
```

---

## Ratio of literary preferences

```{r}
f_lit_mean <- df %>% 
  select(starts_with("F0")) %>%
  transmute(m = rowMeans(.)) %>%
  pull(m)

s_lit_mean <- df %>% 
  select(starts_with("S0")) %>%
  transmute(m = rowMeans(.))  %>%
  pull(m)

f_ratio <- f_lit_mean / (s_lit_mean +f_lit_mean)
```

---

## Principal components

```{r}
s_lit_vars <- df %>% 
  select(starts_with("S0")) %>%
           names()

f_lit_vars <- df %>% 
  select(starts_with("F0")) %>%
  names()

pca_s <- prcomp(df[s_lit_vars], scale = FALSE) 
pca_f <- prcomp(df[f_lit_vars], scale = FALSE)

df <- df %>%
  mutate(
    pc_s1 = pca_s$x[,1],
    pc_s2 = pca_s$x[,2],
    pc_f1 = pca_f$x[,1],
    pc_f2 = pca_f$x[,2]
  )

#remove original f-s variables
df <- df %>%
  select(-f_lit_vars,-s_lit_vars)
```

---

class: inverse, center, middle

# Data partitioning

---

```{r}
library(caret)

df$Customer_Key <- as.character(Customer_Key)

set.seed(42)
index <- createDataPartition(df$Churned30, p = 0.7, list = FALSE)

train_data <- df[ index, ]
test_data  <- df[-index, ]

Customer_Key_train <- train_data$Customer_Key
Customer_Key_test <- test_data$Customer_Key

train_data <- train_data %>% select(-Customer_Key)
test_data  <- test_data  %>% select(-Customer_Key)
```

---

class: inverse, center, middle

# Preprocessing

---

```{r}
library(recipes)

recipe_churn <- recipe(Churned30 ~ ., train_data) %>%
  step_nzv(all_numeric(), -all_outcomes()) %>%
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
  step_corr(all_numeric(), -all_outcomes()) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  prep(data = train_data)

train_data <- bake(recipe_churn, new_data = train_data) %>%
  select(Churned30, everything())

test_data <- bake(recipe_churn, new_data = test_data) %>%
  select(Churned30, everything())

# save recipe
saveRDS(recipe_churn,"recipe_preprocess.rds")
```

---

class: inverse, center, middle

# Training with H2O!

---

### Step 1: start an H2O instance

```{r}
library(h2o)

h2o.init(nthreads = -1)

h2o.no_progress()
```

---

### Step 2: Creating H2O frames

```{r}
train_hf <- as.h2o(train_data)
test_hf <- as.h2o(test_data)

response <- "Churned30"
features <- setdiff(colnames(train_hf), response)

train_hf[, response] <- as.factor(train_hf[, response])
test_hf[, response]  <- as.factor(test_hf[, response])
```

--

### Step 3: Model training with AutoML

```{r echo = F}
aml <- h2o.automl(x = features, 
                  y = response,
                  training_frame   = train_hf,
                  balance_classes  = TRUE,
                  max_runtime_secs = 60*15,
                  nfolds = 10)

```

```{r eval = F}
aml <- h2o.automl(x = features, 
                  y = response,
                  training_frame   = train_hf,
                  balance_classes  = TRUE,
                  max_runtime_secs = 3600*5,
                  stopping_metric  = "AUC", # AUCPR will be available soon
                  sort_metric      = "AUC",
                  nfolds = 10)
```

---

### Ta-Dah!

```{r}
aml@leaderboard %>%
  as_tibble() %>%
  select(model_id, auc)
```

---

### Save all models

```{r eval = F}
for (i in 1:nrow(aml@leaderboard)) {

  aml_model = h2o.getModel(aml@leaderboard[i, 1])
  h2o.saveModel(object = aml_model, "models")
}
```

--

### Load models

If you add the suffix "best" to the filename of the best model, you can load the best model in this way.

```{r eval = F}
model_path <- glue::glue("models/{list.files('models', pattern = 'best')}")

mod <- h2o.loadModel(model_path)
```

---

..Or if you want lo load and compare multiple models ordered by their positions in the leaderboard, you can do it this way. (see "compare_models.R")

```{r eval = F}
#list files
files <- file.info(dir(path = "models", full.names = TRUE), extra_cols = FALSE)

# order by the time of creation
files_ordered <- files[with(files, order(as.POSIXct(mtime))), ]

# filenames
file_names <- rownames(files_ordered)

models <- list()

for (i in files[1:5]) {
  
  mod <- h2o.loadModel(i)
  
  models[i] = mod
}
```

---

class: inverse, center, middle

# Evaluating our model

---

### H2O performance objects

```{r}
mod <- aml@leader

perf <- h2o.performance(mod,test_hf)

perf@metrics$pr_auc

perf@metrics$max_criteria_and_metric_scores
```

---

### H2O Metrics

```{r fig.width = 9}
metrics <- as.data.frame(h2o.metric(perf))

metrics %>%
  select(-tns,-fps,-tps,-fns) %>%
  gather(metric, value, f1:tpr) %>%
  ggplot(aes(x = threshold, y = value, group = metric)) +
  facet_wrap(~ metric, ncol =5, scales = "free") +
  geom_line() +
  theme_minimal()
```

---

### Precision-recall curve

```{r fig.height = 5}
metrics %>%
  ggplot(aes(recall,precision)) + 
  geom_line(size = 1.5) +
  theme_minimal()
```

---

### Confusion matrices

```{r}
h2o.confusionMatrix(perf,metrics = "f2")

h2o.confusionMatrix(perf,metrics = "accuracy")
```

---

### Creating performance plots

```{r}
library(modelplotr)

scores_and_ntiles <- prepare_scores_and_ntiles(datasets=list("test_data"),
                                               models = list("mod"),  
                                               model_labels = list("GBM"), 
                                               target_column="Churned30",
                                               ntiles = 100)

scores_and_ntiles <- scores_and_ntiles %>%
  rename("ntl_0" = ntl_p0,"ntl_1" = ntl_p1)

plot_input <- plotting_scope(prepared_input = scores_and_ntiles)
```

---

### Cumulative gains

*"When we apply the model and select the best X ntiles, what % of the actual target class observations can we expect to target?"*

```{r fig.height = 5}
plot_cumgains(data = plot_input)
```

---

### Cumulative lift

*"When we apply the model and select the best X ntiles, how many times better is that than using no model at all?"*

```{r fig.height = 5}
plot_cumlift(data = plot_input)
```

---

### Cumulative response

*"When we apply the model and select ntile X, what is the expected % of target class observations in that ntile?"*

```{r fig.height = 5}
plot_cumresponse(data = plot_input)
```

---

class: inverse, center, middle

# Model interpretation

---

### Variable importance

```{r fig.height = 5}
h2o.varimp_plot(mod)
```

(See the "EVALUATE.R script for how to plot a decision tree from the model)

---

### Local explanations with LIME

```{r}
library(lime)

explainer <- lime(x = train_data, 
                        model = mod)

explanation <- lime::explain(x = test_data[1:5, ], 
                             explainer = explainer, 
                             labels = "p1",
                             n_features = 3,
                             kernel_width = 0.5)
```

---

```{r}
lime::plot_features(explanation)
```

---

class: inverse, center, middle

# What we didn't cover

---

### Comparing models

Including sorting models based on e.g. AUCPR (see "compare_models.R" script")

--
### Decision tree

See the "EVALUATE.R" for how to plot the model as a decision tree

--
### Predicting on new data

Including changing model predictions according to other thresholds for optimizing e.g. the F2 score (see the "PREDICT.R") script

--
### Clustering churners

Based on model explanations for churn, we can cluster customers according to the variable weights obtained with LIME.

---

class: inverse, center, middle

# Thank You!

### hr.pchristensen@gmail.com

### <https://github.com/PeerChristensen/Churn_autoML>

